%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{csquotes}
\usepackage[fleqn]{amsmath}
\usepackage[11pt]{moresize}
\usepackage{anyfontsize}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{adjustbox}
\usepackage{amssymb}


\usepackage{xcolor}

\usepackage{mdframed}

\usepackage{makecell}


\usepackage{graphicx}
\usepackage{float}
\usepackage{tabu}
\usepackage{pifont}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\newcommand\BibTeX{B\textsc{ib}\TeX}

\definecolor{atomictangerine}{rgb}{0.8, 0.2, 0.1}
\definecolor{turq}{rgb}{0.0, 0.5, 0.5}
\definecolor{darkturq}{rgb}{0.0, 0.4, 0.4}
\definecolor{bright}{rgb}{0.8, 0.1, 0}
\definecolor{darkgray}{gray}{0.3}
\definecolor{gray}{gray}{0.5}
\definecolor{mahogany}{rgb}{0.6, 0.05, 0.05}
% \definecolor{pink}{rgb}{1,0.05,0.6}
\definecolor{myblue}{rgb}{0.3,0.05,0.9}

\definecolor{olive}{rgb}{0.537, 0.627, 0.318}
\definecolor{green}{rgb}{0.22, 0.463, 0.114}
\definecolor{grey}{rgb}{0.4, 0.4, 0.4}
\definecolor{blue}{rgb}{0.435, 0.659, 0.863}
\definecolor{pink}{rgb}{0.761, 0.482, 0.627}
\definecolor{darkpink}{rgb}{0.561, 0.282, 0.427}
\newcommand\darkturq[1]{\textcolor{darkturq}{#1}}
\newcommand\mahogany[1]{\textcolor{mahogany}{#1}}
\newcommand\todo[1]{\textcolor{darkturq}{\textbf{TODO:} #1 }}

\newcommand\darkgray[1]{\textcolor{darkgray}{#1}}

\newcommand\gal[1]{\textcolor{bright}{\textbf{GAL:} #1 }}
\newcommand\yuval[1]{\textcolor{darkpink}{\textbf{YUVAL:} #1 }}
\newcommand\tzuf[1]{\textcolor{blue}{\textbf{TZUF:} #1 }}
\newcommand\reut[1]{\textcolor{green}{\textbf{REUT:} #1 }}

\newcommand\edit[1]{\textcolor{brown}{ #1 }}
\newcommand\ignore[1]{}

\title{ZEST: Zero-shot Learning from Text Descriptions using Textual Similarities and Visual Descriptions}


\author{First Author \\
  Affiliation / Address line 1 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}



We study the problem of recognizing visual entities from text descriptions of their classes. Given images of birds with free-text descriptions of their classes (bird species), we learn to classify images of previously-unseen birds using to previously-unseen class descriptions. This setup has been studied in the vision community under the name {\em zero-shot learning from text}, focusing on learning to transfer knowledge about visual aspects of birds from seen classes to previously-unseen ones. Here, we suggest to focus on the textual description 

%We study the problem of classifying images according to unstructured textual descriptions, in a zero-shot setup. That is, in training we get a set of images each paired with its class category and a corresponding textual-description of that lac.s sIn the zero-shot phase, we get a previously unseen image belonging to a set of previously unseen classes as along with the textual descriptions of these  classes, and we need to decide on the correct class. Previous approaches to this challenge focused on improving the {\em visual} representation, by transforming it to focus on the visually similar and different aspects between classes.
%In contrast, here we propose to focus on the {\em textual} representation, 
and distil the most salient information in the text that would help to effectively match visual features to the parts of the text that  discuss them. 
Specifically,  %(1) we leverage the {\em similarity} between images as it is reflected in texts, that is, when the actual classes of images look similar, the texts tend to be similar too; and
(1) we propose to leverage {\em similarities} of the grounding relation, that is,  the fact that %images as it is reflected in texts, that is, 
the relation of a known bird to its description is indicative of the relation of an unknown bird to its respective description. 
%when the actual classes of images look similar, the texts tend to be similar too; and
(2) we derive {\em visual summaries} of the texts, i.e.,  %that  making the textual description more compatible with
extractive summaries that focus on the {\em visual} features that tend to be reflected in images.
We propose a simple attention-based model augmented with   the  similarity and visual summaries components, and our empirical results  consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based Zero-shot Learning, illustrating the critical importance of texts for zero-shot image-recognition setups.



%We study the problem of classifying images according to unstructured textual descriptions, in a zero-shot setup. That is, during training we see images paired with their classes, and a corresponding textual-description for each class. In the zero-shot phase, we get a previously unseen image, belonging to a set of previously unseen  classes.  Based on the (previously unseen) textual descriptions of these classes, we decide on the correct class. Previous approaches to this challenge focused on improving the {\em visual} representation, by transforming it to focus on the visually similar and different aspects of the classes. In contrast, here we propose to focus  on the {\em textual} representation, and distill the most salient features for the classification. Specifically (1) we leverage the similarity between images as it is reflected in texts, that is, when the actual classes of  images look similar, the texts are  similar too; and (2) we provide {\em visual summaries} of the text, making the text more compatible with the features that tend to be reflected in images. Our results for a simple attention-based model augmented with the textual similarity and visual summarization components   consistently and significantly outperform the state-of-the-art on multiple largest benchmarks for Text-based Zero-shot Learning, illustrating the critical importance of the texts for zero-shot image-recognition setups. 
\end{abstract}




\section{Introduction}

In computer vision, {\em zero shot-learning} (ZSL) for image classification is the problem of classifying images given auxiliary information. The image classification model is trained to classify images from a known, pre-defined set of classes. During test time, images belonging to new classes are given, and the task is to transfer knowledge learned from seen classes during training to  unseen classes that are provided at test time. 
%\gal{break to shorter sentences}



\begin{figure}[t]
\centering
\scalebox{0.48}{
\includegraphics[width=\textwidth]{images/text_similarities.png}}
 \caption{Textual similarity and visually relevant-descriptions enhancing: (1) Our approach leverages the similarity within %\gal{visually-relevant} \tzuf{no}
 texts (red) via document clustering (bottom box - representing documents clustered and labeled alphabetically); %\gal{clustering of what?};
 (2) non-visually relevant description are removed (blue text).} %\gal{Clarify in the figure that the text is class description, not image description. Clarify what  A,B<C stands for, Clarify what is the space in which the letters are located.}}
\label{fig:bird_example}%
\end{figure}

A common setup for ZSL assumes that the auxiliary information is a set of semantically meaningful properties (called attributes) for describing the class (e.g., black-beak, long-tail, etc.) \citep{farhadi2009describing,lampert2009learning}. Typically, these attributes are  manually collected by human raters for each image (test and train alike), and averaging across images.
%The class description is then produced by averaging scores per attribute in all images that belong to the class \gal{not necessarily}. 
%This process is problematic, as the human raters are exposed to images from the test-set --- which defeats the very  purpose of the ZSL setup. \gal{I'm not sure that you want to attack everyone in the field on the 2nd paragraph} \tzuf{wasn't this Yuval's point which you agreed upon?}

%\citep{farhadi2009describing, lampert2009learning, atzmon2018probabilistic,akata2013label,wang2013unified,changpinyo2016synthesized, akata2015label,ji2018stacked}. \par



A more realistic approach \cite{elhoseiny2017link}%\gal{add refs} 
relies on available online text that describes classes. It thus avoids the expensive annotation and exposure to images from the test-set.
Thus, the input of the task has been revised to be (1) an image; and (2) the text descriptions of the classes. % as the auxiliary information. \gal{revise last sentence}

%\gal{Here you want to explain the main challenges in from wiki descriptions: That most of the text is irrelevant to bird appearance, and only a few sentences are relevant.}

In this work we classify birds species according to wiki-like descriptions.
This task raises many challenges: 
(1) Differences between the birds are very small, which makes ours a very fine-grained classification task; (2) This is an expert task, %as in addition to the small difference between the birds, the 
text contains terminology for describing a bird that is not likely to be familiar to a layman; and, on top of that 
(3) The text descriptions of the classes are long, containing few visually relevant sentences; 
%(3) The text descriptions of the classes are long and very noisy.
Furthermore, one can  address this ZSL tasks in two scenarios: %The ZSL task contains two real scenarios:
(i) an \enquote{easy} scenario where we see a new unknown object that is similar to a previously known object; (2) a \enquote{hard} scenario, where the new object is very different from known objects, i.e., belonging to an entirely different family or species than those observed during training.

%thus, the relevance between seen and unseen objects is minimal.\par

In this work we focus on text representation and address a key question in ZSL, namely, how can we detect aspects of the text which would be important for classifying images? or \textit {how can we identify text components that are visual in nature?}
The key idea, in a nutshell, is as follows: (1) leverage the similarities between the seen and unseen (e.g., having tail and hooves), and (2) identify salient visually relevant descriptors (e.g., black and white stripes) in the previously unseen classes descriptions.  %Thus, in an easy scenario where the horse is known to us but the zebra is not - we can benefit from the similarity between the objects and texts. Whereas in a hard scenario we know neither the class of horses or zebras, and we want to classify a new image as horse or zebra. This requires that we enhance the visual differences in the text.\par


%To get an intuition on the task, 
%\gal{(1) This feels out of place. Either move    to earlier paragraph or move to later  when you explain the idea of similarity and embedding.  } \tzuf{Reut and I have been back and forth on the position of this paragraph. I don't see else where it can belong and it is important as people from NLP just don't get this task.}
%consider the following situation:
Imagine you never saw a zebra in your life but know how a horse looks like. What if you were given a text describing a zebra: \enquote{Zebras have hooves, mane, tail, pointed ears, and white and black stripes}. This description would probably be very close to a description of a horse having \enquote{hooves, mane, tail, pointed ears} and you would probably be looking for an image that reminds you of a horse but has \enquote{white and black stripes}. Thus, without previously knowing what a zebra is, using text-descriptions of the zebra and  knowledge already acquired on horses, one can correctly classify unknown categories like a zebra.

%which aspects of the text are important for classifying images?
We use the intuition that similar objects (images) have similar texts to create a kernel-like method \gal{I think this is not kernel-like after all} to enhance separability of texts that reflects real similarity of the objects; and we leverage the intuition that the differences would be the most salient visual features, by extracting visually relevant  descriptions from the text, thus enhancing the compatibility of the text and the image (see Figure \ref{fig:bird_example}). 



Our experiments empirically demonstrate both the {\em efficacy} and  {\em generalization} capacity of our proposed solution. 
In experiments carried out on two large ZSL datasets, %\gal{not exactly}
in both the {\em easy} and {\em hard} scenarios, the similarity method obtains a ratio improvement of up to 18.3\%, and with the addition of extracting visually relevant description, we obtain a ratio improvement of up to 48.16\% over the state-of-the-art.
%Furthermore, we show the generality of our 
We further show that our visual-summerization method generalizes across models and datasets, and we demonstrate its contribution to additional SOTA Models, by a ratio improvement of up to 59.62\%.



%Our main contributions are as follows:  
The contribution of this paper are three-fold.
%\begin{enumerate} \item 
First,  to the best of our knowledge we present the importance of text in zero-shot image recognition by showing two distinct text-based processing methods that vastly improve the results.
Secondly, we demonstrate the efficacy and generalizability of the methods by applying them to the zero-shot generalized-zero shot tasks, out preforming all previously reported results on the CUB and NAB Benchmarks. Finally, we show that visual aspects learned from one dataset can transfer effectively  to another dataset without the need to obtain further dataset-specific annotations.  


%\end{enumerate}

The significance of the improvement of our text processing methods, on a variety of models, implies value in applying text processing methods in future research, in tasks which include vision and language modalities. \gal{revise.}


\section{Background and Related Work}

%Recent successes of deep learning methods for the task of object recognition are heavily dependent on large dataset collection.
%Moreover, the number of categories keeps increasing and with it the need to collect more data. However, this process of collecting millions of images and their corresponding tags is highly expensive and time-consuming. \par

Zero-shot learning (ZSL) aims at overcoming the need to label massive datasets for new categories, by learning the connections between images and prior auxiliary knowledge about their classes. At  test-time, this auxiliary information compensates for lacking visual information about new categories.
%Thus, the input is an image and auxiliary information about the classes, and the output is the class label. In the zero-shot phase, new classes are given, unseen during training. In the ZSL, the auxiliary information's function is to compensate for the unseen-before visual information.
\par

Text-based ZSL is a specific multimodal instantiation of this learning task. It is composed of three parts: (1) text representation (2) image-representation (3) a compatibility function between the two modalities: image and text.   Previous work focused mainly on the latter two parts \cite{zhu2018generative, akata2015evaluation}. Here we focus on the text modality.  

Many ZSL  studies \cite{xu2018attngan,lei2015predicting,qiao2016less,akata2016multi}  for object recognition, which look into the image modality, rely on features  extracted using the well-established method of
Convolutional Neural Network (CNN) \cite{lecun1995convolutional}. More recent studies use object detection methods to detect parts of the object and extract visual features at the part-level \cite{elhoseiny2017link,zhu2018generative,zhang2016spda}. This
method makes the image more compatible with the text, as it enables text-terms such as \enquote{crest} to be linked to the visual representation of parts like \enquote{head}. 



%There are different approaches as to the function that learns compatibility function between images and their textual descriptions: (1) \textbf{Visual-to-Semantics:} learning a function that maps from the visual space to the semantic space \citep{socher2013zero}. Thus, if one has an image, the function predicts the correct text related $F(X)=Y$ where X is the images and Y the texts; (2) \textbf{Semantics-to-Visual:} learning a function from the semantic space to the visual space \citet{zhu2018generative} $F(Y)=X$. This can be done by generating images from text, thus, the problem becomes a \enquote*{regular} classification problem as we now have generated images with the corresponding text labels; (3) \textbf{Joint learning:} simultaneously learns a function from the semantic and visual space to a common space and predicts a compatibility score: $F(X,Y)=score$  \cite{akata2015evaluation,akata2016multi,qiao2016less,elhoseiny2013write,elhoseiny2016write}.\par



In this work, we adopt the third approach and aim to process the textual modality in order to increase the compatibility between textual features and visual features, that can be detected by the learning architecture.
\par

Most studies to date have used an impoverished text representation such as Bag-of-Words (BOW) and Term Frequency-Inverse Document Frequency (TF-IDF) and focused on improving the visual representation and the model \cite{lei2015predicting,elhoseiny2013write,elhoseiny2016write,elhoseiny2017link,zhu2018generative}. Beyond that, \citet{qiao2016less} used a simple BOW and a l2,1-norm based objective function to suppress the noisy signal in the text. However, this impoverished treatment of the text side is problematic, as the text contains key information to the correct class. 

In this work we aim to establish the importance of adequately processing the text side, to obtain a sound representation of the features most salient for the task. This is done to complement the work done on the visual modality, that aimed to increase image compatibility with the text.

%\section{Zero-shot Learning (ZSL) Background}




\section{Proposed Approach}
\label{task}

The key idea underlying our proposed solution is to transform the text representation from a non-task-oriented text representation to a task-oriented representation  focused on the most salient features for the visual recognition task. More specifically, instead of letting the text representation encode a broad class description, we designed a representation that encodes visual features that are relevant for the classification. We do so by employing two different (complementary) methods: (1) text similarities; and (2) extraction of visual-description from the text.




\begin{figure*}[th]
\centering
\scalebox{1}{
\includegraphics[width=\textwidth]{images/ZEST_model6.png}}
 \caption{Our ZEST$_{similarity}$+VS model with similarity component and Visually-Focused Summarization (VS) component.}
\label{fig:data}
\end{figure*}
\subsection{Basic Architecture (ZEST$_{vanilla}$)}
\label{section_ZEST_C}


Our basic, vanilla, architecture is a simple multiplicative attention mechanism \cite{luong2015effective} inspired by \citet{romera2015embarrassingly}. We model the problem using an attention-based model, where the image $x$ is queried
%considered 
against the candidate documents of the classes $D$ .
Formally, let $X\in \mathbb{R}^{N\times m}$ be a set of N image feature vector $x_1,\ldots,x_N$.
Let  $D\in \mathbb{R}^{N'\times m'}$ be a set of N' document feature vector  $d_1,\ldots,d_N'$.  %\gal{define d ?}.
 \(W\in  \mathbb{R}^{m\times m'}\) is a learned matrix.  Formally, the label $\hat{y}$ assignment of an image $x$ is defined as:
 
%\odot 
\begin{equation}
    \label{equation:attention}
    \hat{y}={argmax}_{d\in D}x^TWd
\end{equation}



We learn matrix $W$ using categorical cross-entropy with class label $y$: 
\begin{equation}
    \label{equation:attention}
    \mathcal{L(W)}=-x^TWd_y  +\log\left(\sum_{i=1}^{N'} \exp(x^TWd_i)  \right)
\end{equation}


%\gal{this doesnt make sense. learning or inference? Define the inference problem first, and then define the learning problem.}


%Where $d\in D$ is the document in the set of documents $D$ that receives the maximum score on the dot product with $x$. 



\begin{figure}[t]
\centering
\scalebox{0.42}{
\includegraphics[width=\textwidth]{images/DS_model.png}}
 \caption{Deterministic Similarity (DS) Model}
\label{fig:DS}
\end{figure}

\subsubsection{Image Encoder}
\label{section:Image_Encoder}
The goal of the visual encoder is to transform the image to a vector representation of the most salient visual features for the classification.\par

We adopt a 
Fast Region-based Convolutional Network framework (Fast R-CNN) for object detection   \citep{girshick2015fast} to detect seven semantic parts in the CUB dataset:
\enquote{head},\enquote{back},\enquote{belly},\enquote{breast},\enquote{leg},\enquote{wing},\enquote{tail}.\par
This image-representation method was present by \citet{zhang2016spda} and applied to the text-based ZSL task \cite{zhu2018generative,elhoseiny2017link} . \par

The images are first encoded by VGG16 architecture \citep{simonyan2014very} and then visual features are extracted from parts detected using Region of Interest Pooling pooling (ROI) layer \citep{girshick2015fast}. %add cite to ROI pooling
The encoded features of each visual part are then concatenated into a feature vector which functions as the image representation for the text-based ZSL.


\subsubsection{Basic Text Encoder}
\label{section:Text_Encoder}
Our basic encoder processes the text into a feature vector. It is a Term Frequency-Inverse Document Frequency (TF-IDF)  representation \citep{salton1988term}, which requires the following pre-processing: (1) tokenization of the words; (2) stop words are removed; (3) stemming is performed on the remaining words; (4) Finally, we extract a feature vector using TF-IDF. This processing procedure is similar to the text processing presented by \citet{zhu2018generative}.
The dimensionalities of TF-IDF features for
CUB and NAB are 7551 and 13217 respectively. 


\subsection{The Importance of being Similar}

We leverage the similarities between images and texts. Meaning, when the actual classes of the images look similar, the texts are also similar.

Similarities between texts reflect on real similarities between objects. We want to reconstruct this analogy-link.


The TF-IDF representation of a text is on the word-level: representing the uniqueness of a word in the document and between documents. However, this word-level representation of similarity does not take document level similarities into account. \par

\par

Our intuition is that similarities between texts will suggest a similarity between objects, and vice versa. To test this intuition we propose two models: (1) a deterministic model (2) adding a similarity component to our stochastic model ZEST$_{vanilla}$.

For both models we use the Image Encoder (section \ref{section:Image_Encoder} to process the images $x$, and the Text Encoder (section \ref{section:Text_Encoder}) to process documents D.

\subsubsection{Deterministic Similarity (DS) Model}

Our approach leverages the analogy between images and texts. Figure \ref{fig:DS} presents our Deterministic Similarity (DS) Model. Given an image $x^{zero}$ in the zero-shot phase we: (i) look for the cosine similarity between $x^{zero}$ and all the images from training $X^{train}$. We get image $x_k^{train}$ from class $k$; (ii) $d_k^{train}$ is the document describing class $k$  (iii)
look for the cosine similarity between $d_k^{train}$ and all the texts from zero-shot phase $D^{zero}$. We achieve the document $d_y^{zero}$ which corresponds to the class label $y$:


The following equation follows step (i):

\begin{equation}
\begin{aligned}
\begin{split}
&k= \\
&\operatorname*{argmax}_{x^{train}_k} \frac{<x^{zero},x^{train}_k>}{\| x^{zero}\| *\| X^{train}\| } %, k = 1 : K
\end{split}
\end{aligned}
\end{equation}

The following equation follows step (iii):

\begin{equation}
\begin{aligned}
\begin{split}
&y=\operatorname*{argmax}_{d_y^{zero}\in D^{zero}} \frac{<d^{train}_k,d_y^{zero}>}{\| d_k^{train}\| *\| D^{zero}\| } %, y = 1 : Y
\end{split}
\end{aligned}
\end{equation}

\subsubsection{ZEST$_{similarity}$}

Having established the importance of similarity, here we embed it into our stochastic ZEST$_{vanilla}$, to benefit from it in the learning procedure. \par

We take a kernel-like approach, aiming to transform the text space into a linearly separable space, where sets of texts are separable. \par

The Basic Encoder used allows us to see similarities and differences on the word-level. However, we want to find similarities on the document level. To achieve that, we add our $similarity$ component which clusters training and test texts. If over 15\% of the texts from test-set are clustered with at least one text from train-set we concatenate the encoded cluster label to the document representation $d$. \par

This similarity-difference representation is then passed as an input to the ZEST$_{vanilla}$ model (\ref{section_ZEST_C}). \par

We hypothesize that the similarity component will work well on the \enquote{easy} scenario - where closely related birds are seen during training. \par

\subsection{The Importance of Being Seen (ZEST$_{similarity}$+VS)}

While the similarity method takes advantage of the similarity between objects seen in training and test, here we want to address the scenario where similar objects are seen during test time only (e.g. zebras and mules). To differentiate between closely related classes in the test-set we need to emphasize the parts that are both different in the image and the text - the visual features. \par

Our aim here is to extract salient visual features in the text, thus, making the text more compatible with the image. 

 \par
 
\subsubsection{Visually-Focused Summarization (VS)}


Our method for enhancing visual description is based on a Visually-Focused Extractive Summarization (VS).

Extractive summarization is the task of extracting a small number of sentences that summarize a given document.
In this work we define: Visually-Focused Extractive Summarization (VS) as the task of extracting only sentences which are visually relevant language.The term \textit{Visually Relevant Language (VRL)} was coined by \citet{winn2016detecting} to indicate sentences which are visually descriptive with respect to the object (i.e., bird species).


A \enquote{naive} approach would be to extract sentences that we know are visually salient in our domain (e.g., the 7 parts employed by the vision recognition representation).  However, this naive approach has the following drawbacks: (1) bird parts can be described in many different terms; (2) a bird can be described by its color, without any mention of a body-part. 
Instead, we propose to use similarity between sentences containing visual-descriptions. According to this approach, if we want to extract sentences that contain visual-descriptions than we need to compare the similarity between sentences in the document and other sentences that happen to visually describe these kinds of objects. 
\par
Note that we cannot rely on descriptions of particular species (due to the zero-shot setup), and have to do with descriptions of objects in the family that can be naturally obtained for the general class of objects we are interested in.\par

Such naturally occurring descriptions are, for instance, captions of bird images. (not necessarily those species from our dataset). We use a set of captions to create an unsupervised classifier that predicts whether a sentence is relevant, that is, whether it contains descriptions that can be seen in an image.
%We use only captions of birds that are seen during training.
For each document we do pairwise similarity between captions and sentences and assign a score to each sentence.

We embed both captions ($c_{0:N}$) and sentences ($s_0:M$) in the document with BERT \cite{devlin2019bert} and a pre-trained siamese network framework developed by \citet{reimers2019sentence}. The score of sentence $s$ is defined as:

\begin{equation}
\begin{aligned}
\begin{split}
\label{eq:score}
&score(s)=  \frac{1}{N}
&\sum_{i=0}^{N} \frac{c^i\cdot s}{\| s\| }
\end{split}
\end{aligned}
\end{equation}

 We then extract only the top 30 sentences or $\frac{3}{4}$ of the sentences in the document, whichever is lower.\par   

\par




Our documents contain many non-visually relevant descriptions that are unobserved in the images. However, these non-visually relevant descriptions might still be important to the similarity comparison of the documents (e.g. similar-looking birds are likely to be in the same habitat). Thus, the visual-descriptions extraction and the similarity enhancing are done parallel on the original document. We then concatenate the similarity embedding and the visual-descriptions embedding of the text and perform the multiplicative attention on the encoded documents and image.

\section{Experiments}

\subsection{Experiment setting}

\paragraph{Datasets:}  We evaluate our method on the Caltech UCSD Birds-2011 dataset (CUB) \citet{wah2011caltech} and the North Americaâ€™s birds dataset (NAB) \cite{van2015building}, using class description obtained from wikipedia and AllaboutBirds website \cite{AllaboutBirds} collected by \citet{elhoseiny2017link}. 
Both are datasets of birds, with small differences in the images and texts, making it a challenging task of fine-grained classification. The CUB dataset contains 11,788 images of 200 bird species, and the NAB
is a larger dataset of birds with 48,562 images of 404 classes \footnote{\citet{elhoseiny2017link} merged the original 1,011 classes according to the subtle division of classes.}.
In addition, the texts of both CUB and NAB are long, containing much irrelevant information. CUB has an average of 869 tokens and 42 sentences per document of a class. NAB has an average of 1277 tokens and 58 sentences per document of a class. 

\paragraph{Two split Settings} We use the two splits for both datasets presented by \citet{elhoseiny2017link}: (1) Super Category-Shared (SCS) also referred to as the \enquote*{easy} split; and (2) Super-Category-Exclusive (SCE) also referred to as the \enquote*{hard} split. The SCS-split for each class in the test-set, at one class in the training belongs to the same category (categories are organized taxonomically). E.g. in Figure \ref{fig:bird_example}, the Rufous Hummingbird and the Ruby-throated Hummingbird are both from the Hummingbird category. In the SCE-split scenario, all the classes in a category are in the same set, such that, if a class is in the test-set then other classes from the same category will be in the test-set. Intuitively, classes from the same category have high similarity in both images and texts, so that in the SCE-split scenario a class is seen for the first time but a very similar looking bird was seen in training, making it easier than the SCS.

\paragraph{Clustering Algorithms:}
The parameters of our model include clusters parameters.
We use two clusters methods: (1) Density-based spatial clustering of applications with noise (DBSCAN) \citep{ester1996density}; (2) Hierarchical DBSCAN \citep{mcinnes2017hdbscan}.
We optimize the parameters on a validation set. All variants of the ZEST model are trained using categorical cross-entropy.
%optimization loss

\paragraph{Human Summarization:} 
In order to test our visual-description extraction approach, we designed an oracle experiment using ground-truth visually-focused summarization. To this end, we manually annotated the CUB dataset by extracting sentences which include visually relevant descriptions of the bird subject. Only 11.9\% of the sentences were found to include visually relevant descriptions.  

\paragraph{Image Captions:}
To create visual summaries we use image captions of birds from the CUB train-set, provided by \citet{reed2016learning}. To showcase the generality of this approach we use these captions in both in-domain (CUB) and out-of-domain (NAB) scenarios.
In both datasets, we avoid using captions of unseen bird class. In addition, only models which include Image captions VS component (+VS), use image captions.\par

Each image in the CUB dataset has been annotated with 5 fine-grained captions. These captions describe only the visual appearance of the birds while avoiding mentioning the name of the bird specie. E.g., \enquote{This bird has a long beak, a creamy breast, and body, with brown wings}. 

\par

%We limit the use of caption to those that correspond to  mages that are seen during training in the CUB dataset. In the NABirds all caption from the CUB dataset are allowed.


\paragraph{Baselines:}  The performance of
our approach is compared to ten leading algorithms (see Table \ref{tab:results}): MCZSL\citep{akata2016multi}, WAC-Linear \citep{elhoseiny2013write}, Wac-Kernel \citep{elhoseiny2016write}, ESZSL \citep{romera2015embarrassingly}, SJE \citep{akata2015evaluation}, Sync$_{fast}$ \citep{changpinyo2016synthesized}, Sync$_{OVO}$ \citep{changpinyo2016synthesized},
ZSLNS \citep{qiao2016less}, and GAZSL \citep{zhu2018generative}. 


\paragraph{Generalized Zero-Shot Learning:} 
The conventional zero-shot learning task considers only unseen classes during zero-shot phase. However, in a realistic scenario seen objects might also appear \cite{chao2016empirical}. In Generalized Zero-Shot Learning (GZSL), test data might also come from seen classes and the labeling space is the union of both types of seen and unseen classes. GZSL is this considered a more formidable problem setting
than ZSL due to the model's bias towards the seen classes.\par
 

We follow the metric present by \citet{chao2016empirical}, to evaluate our models on the GZSL task. We evaluate the accuracy of a Seen-Unseen accuracy Curve (SUC) and use Area Under  SUC to measure the general capability of ZSL methods.

 \begin{table}[t]
  \centering
 \scalebox{0.6}{

\begin{tabular}{l|ll|ll} 
\Xhline{6\arrayrulewidth}
\multirow{2}{*}{\textbf{ methods} }                                  & \multicolumn{2}{c|}{\textbf{CUB } }                                    & \multicolumn{2}{c}{\textbf{NAB} }                                     \\ 
\cline{2-5}
                                                                     & \multicolumn{1}{c}{\textbf{SCS} } & \multicolumn{1}{c|}{\textbf{SCE} } & \multicolumn{1}{c}{\textbf{SCS} } & \multicolumn{1}{c}{\textbf{SCE} }  \\ 
\hline
Random classification                                                          & 2                                 & 2.5                                & 1.23                              & 1.23                               \\ 
\hline
MCZSL \citet{akata2016multi}                        & 34.7                              & -                                  & -                                 & -                                  \\
WAC-Linear \citet{elhoseiny2013write}          & 27.0                              & 5.0                                & -                                 & -                                  \\
WAC-Kernel \citet{elhoseiny2016write}              & 33.5                              & 7.7                                & 11.4                              & 6.0                                \\
ESZSL \citet{romera2015embarrassingly}              & 28.5                              & 7.4                                & 24.3                              & 6.3                                \\
SJE \citet{akata2015evaluation}                    & 29.9                              & -                                  & -                                 & -                                  \\
ZSLNS \citet{qiao2016less}                          & 29.1                              & 7.3                                & 24.5                              & 6.8                                \\
SynC$_{fast}$ \citet{changpinyo2016synthesized}     & 28.0                              & 8.6                                & 18.4                              & 3.8                                \\
SynC$_{OVO}$ \citet{changpinyo2016synthesized}      & 12.5                              & 5.9                                & -                                 & -                                  \\
ZSLPP \citet{elhoseiny2017link}                   & 37.2                              & 9.7                                & 30.3                              & 8.1                                \\
GAZSL \citet{zhu2018generative}                   & 43.7                              & 10.3                               & 35.6                              & 8.6                                \\ 
\hline
Deterministic Similarity (DS) & 40.402                            & 5.551                              & 37.002                            & 5.517                              \\
ZEST$_{vanilla}$                                                                 & 39.16                            & 11.77                             & 27.61                            & 10.18                             \\
\textbf{ZEST$_{similarity}$}                                           & \textbf{47.48}                   & \textbf{11.77}                     & \textbf{38.2}                   & \textbf{10.18}                    \\
\hline
\textbf{ZEST$_{similarity}$+VS}                                        & \textbf{48.57}                    & \textbf{15.26}                    & \textbf{38.51 }                   & \textbf{10.23 }                    \\ 
\Xhline{6\arrayrulewidth}
                 
\end{tabular}
}
\caption{Top-1 accuracy (\%) on CUB and NAB datasets with two split settings.}
\label{tab:results}
\end{table}


\begin{table}[t]
\centering
 \scalebox{0.61}{
\begin{tabular}{l|cc|cc} 
\Xhline{6\arrayrulewidth}
\multirow{2}{*}{\textbf{ methods} } & \multicolumn{2}{c|}{\textbf{CUB } }                                                              & \multicolumn{2}{c}{\textbf{NAB} }           \\ 
\cline{2-5}
                                    & \textbf{SCS}                                    & \textbf{SCE}                                   & \textbf{SCS}         & \textbf{SCE}          \\ 
\hline
GAZSL                               & \begin{tabular}[c]{@{}c@{}}43.74\\\end{tabular} & \begin{tabular}[c]{@{}c@{}}10.3\\\end{tabular} & 35.6                 & 8.6                   \\
GAZSL+parts summarization                               & \begin{tabular}[c]{@{}c@{}}19.54\\\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.557\\\end{tabular} & 23.32
                 & 7.2
                   \\
GAZSL+parts summarization +${similarity}$                               & \begin{tabular}[c]{@{}c@{}}38.25\\\end{tabular} & \begin{tabular}[c]{@{}c@{}}9.557\\\end{tabular} & 33.05
                 & 7.2
                   \\
GAZSL+VS                            & 43.72                                           & \textbf{16.44}                                       & 37.28
                & 9.237                 \\
GAZSL+HUMAN                         & 35.98                                           & 21.81                                          & -                    & -                     \\
GAZSL+HUMAN+${similarity}$                       & 47.32                                           & 21.81                                          & -                    & -                     \\ 
\hline
ZEST$_{similarity}$                                & 47.48                                           & 11.77                                          & 38.2
                & 10.18                 \\
ZEST$_{similarity}$+parts summarization                                & 42.27
                                           & 10.93
                                          & 37.02

                & 8.055
                 \\
            ZEST$_{vanilla}$                                                                 & 39.16                            & 11.77                             & 27.61                            & 10.18                             \\
                
                 
  ZEST$_{vanilla}$+VS                             & {42.58}                                  & {15.26}                                 & {32.24}      & {10.23}        \\
ZEST$_{similarity}$+VS                             & \textbf{48.57}                                  & {15.26}                                 & \textbf{38.51}      & \textbf{10.23}        \\

ZEST$_{similarity}$+HUMAN                          & 48.99                                & 17.2                                           & -                    & -                     \\ 
\Xhline{6\arrayrulewidth}
\end{tabular}
}
    \caption{Visually-Focused Summarization (VS) with GAZSL, ZEST$_{vanilla}$, and ZEST$_{similarity}$.  }%
    \label{table:summarization}%
\end{table}





\begin{table}[t]
\centering
 \scalebox{0.87}{
\begin{tabular}{l|c|c} 
\Xhline{6\arrayrulewidth}
    \multirow{2}{*}{ \textbf{methods }} & \textbf{CUB }  & \textbf{NAB }   \\ 
\cline{2-3}
                                    & \textbf{SCS }  & \textbf{SCS}   \\ 
\hline
ZEST$_{vanilla}$                     & 39.16          & 27.61          \\
ZEST$_{vanilla}$ + bird category  & 43.71          & 36.73           \\
Zest$_{vanilla}$ + 1 cluster             & 46.55          & 35.94           \\
Zest$_{similarity}$ (with 2 cluster) & \textbf{47.48} & \textbf{38.2}   \\
\Xhline{6\arrayrulewidth}
\end{tabular}
}
    \caption{Zest model with different Similarity methods }%
    \label{table:similarity}%
\end{table}




\begin{table}[t]
\centering
 \scalebox{0.76}{
\begin{tabular}{l|cc|cc} 
\Xhline{6\arrayrulewidth}
                  \multirow{2}{*}{\textbf{ methods} } & \multicolumn{2}{c|}{\textbf{CUB } }                                                              & \multicolumn{2}{c}{\textbf{NAB} }           \\ 
\cline{2-5}
                                    & \textbf{SCS}                                    & \textbf{SCE}                                   & \textbf{SCS}         & \textbf{SCE}          \\ 
\hline
ESZSL             & 0.185           & 0.045             & 0.092           & 0.029             \\
ZSLNS             & 0.147           & 0.044             & 0.093           & 0.023             \\
WAC$_{kernal}$         & 0.225           & 0.054             & 0.007           & 0.023             \\
WAC$_{linear}$         & 0.239           & 0.049             & 0.235           & -                 \\
SynC$_{fast}$          & 0.131           & 0.040             & 0.027           & 0.008             \\
SynC$_{OvO}$           & 0.017           & 0.010             & 0.001           & -                 \\
ZSLPP             & 0.304           & 0.061             & 0.126           & 0.035             \\
GAZSL             & 0.354           & 0.087             & 0.204           & 0.058             \\ 
\hline
ZEST$_{similarity}$              & \textbf{0.443 } & \textbf{0.1 }     & \textbf{0.267 } & \textbf{0.067 }   \\
\hline
ZEST$_{similarity}$+VS           & 0.437           & 0.147             & 0.26            & 0.084             \\
ZEST$_{similarity}$+HUMAN        & 0.445           & 0.163             & -               & -                 \\
\Xhline{6\arrayrulewidth}
\end{tabular}
}
    \caption{Generalized Zero-Shot Learning: AUC of Seen-Unseen Curve.}%
    \label{table:GZSL}%
\end{table}


\subsection{Results}
Table \ref{tab:results} presents the top-1 accuracy. 
The table is divided into four sections, which are (from top to bottom): 
(1) random classification; (2) previous work; (3) our models with previous setup (for comparison to previous work); (4) Our model with additional data - captions. \par 

\paragraph{DS Model:} 
According to table \ref{tab:results}, the DS model achieves competitive results on the SCS-split - 40.402\% and 37.002\% on CUB and NAB correspondingly. The high scores on the SCS-split, where similar birds have been seen during training, is expected - as this method relies on similarities within texts and images.\par

In contrast, the DS suffers from low accuracy on the hard-split, where different categories of birds have been seen  during training. As the DS model relies on text and image similarities, the low accuracy suggests that birds from different categories are less likely to look alike - which is intuitive. 

\paragraph{ZEST$_{vanilla}$}
In contrast to the very sophisticated approaches of \citet{zhu2018generative}, our simple stochastic text-based approach outperforms all previous methods on the SCE-split on both CUB (+14.27\% ratio of improvement) and NAB (+18.37\%  ratio of improvement). As the SCE-split is a more challenging split, this sheds light on the strength of this simple framework.  

\paragraph{ZEST$_{similarity}$}
In order to combine the
Next we embed the similarity approach seen in the DS model into the stochastic system as we want to combine the strength of the ZEST$_{vanilla}$ on the SCE-split with the similarity strength presented in the DS model on the SCS-split.
The ZEST$_{similarity}$ model adds the similarity to the representation of the text-only when similar texts are found. Thus, in the case of the SCE-split no similarities are found, and the ZEST$_{similarity}$ preforms as the ZEST$_{vanilla}$ model. 
The combination of a simple yet strong framework as the ZEST$_{vanilla}$ that achieves state-of-art of the SCE-split and the similarity addition- achieves state-of-the-art on all split, in both CUB and NAB datasets. \par


The two clustering algorithms we applied find real similarities, achieving high accuracy in predicting the correct label according to the taxonomic category - 88\% and 84.5\% accuracy on the CUB, and 93.07\% and 95.05\% on the NAB. 

In Table \ref{table:similarity} we can see a comparison between different similarity enhancing methods. The use of two clusters that capture different similarities improves our model over an embedding of the bird category in the text representation, by an absolute improvement of up to 3.77\%.
\par

In Table \ref{table:GZSL} we present the results of ZEST$_{similarity}$ on the GZSL setup. On both datasets and splits, the ZEST$_{similarity}$ achieves the state-of-the-art with up to 30.88\% ratio improvement.

\paragraph{ZEST$_{vanilla}$+VS and ZEST$_{similarity}$+VS }
The ZEST$_{vanilla}$+VS and  ZEST$_{similarity}$+VS models use the captions from training images in the CUB. 

We test the summarized representation on the ZEST$_{vanilla}$ model, the ZEST$_{similarity}$ and the GAZSL \citep{zhu2018generative} model. In Table \ref{table:summarization} we show the experimental results. We compare the models before and after the use of the Visually-Focused  Extractive Summarization component. We see an improvement in accuracy in both models on both datasets and on both splits. The GAZSL improves especially on the SCE-split, as the similarity between texts is reduced. In contrast, the ZEST$_{similarity}$ with the similarity component handles the SCS-split, having a component that enhances the similarity in the text representation.  \par

In addition, we see a drop in the \citep{zhu2018generative} performance on the SCS-split. This is because the summarization process of the text reduces the similarity which is important in the SCS-split. However, when adding our similarity model to the \citep{zhu2018generative} with human text summarized (GAZSL+HUMAN+$similarity$) the performance goes up again.
\par  

To assess the quality of the VS summarization performance we treat HUMAN summarization as the ground truth. The VS method succeeds in removing 49.4\% of the sentences in on the CUB dataset with 96.23\% recall and 22.59\% precision. For comparison, removing 49.4\% of the sentences randomly produces a recall of 50.6\% and a precision of 11.9\%.
Another comparison we made is to a state-of-the-art extraction summarization model such as  Textrank \citep{mihalcea2004textrank} algorithm, which achieves a recall of 12.33\% and a precision of 12.83\%.

\par
We then compare both ZEST$_{similarity}$ and the GAZSL to the use of HUMAN summarization in the CUB dataset and see additional improvement in both models on the two splits. The gap between the performance on the Visually-focused summarization and the Human summarization indicates that improvement in the summarization of documents will improve the models' performance, and is, therefore, a promising path for text-based zero-shot learning research.\par


\begin{figure}[t]
\centering
\scalebox{0.47}{
\includegraphics[width=\textwidth]{images/captopm_graph_3.png}}
 \caption{Accuracy per the number of captions used for summarization, measured on the hard Super-Category-Exclusive split (SCE) of CUB. It demonstrates that even 5 captions are sufficient to guide the visually-focused summarization module.}
\label{fig:captions}
\end{figure}


Finally we experiment to assess the number of captions needed for the VS method. The results are seen in Table \ref{fig:captions}. The results show that only a few (~5) captions are needed to achieve the maximum accuracy with this method.  Thus, the VS method does not require using crowdsourcing to produce image captions. 



\par



\section{Conclusion}
\label{conclusion}

This work aimed to establish a better way to represent the text, one which will support text-based ZSL task. Our text-processing methods' significant achievements, prove that text-processing methods are an essential process in text-based ZSL tasks, or indeed, in any vision and language-based tasks. We thus, recommend applying text-processing methods in future research which includes vision and language modalities.


\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}
\end{document}
